sdocp('sdoc.js.sdoc', 'SDoc JavaScript driver | Spencer Tipping\nLicensed under the terms of the MIT source code license\n\nvar sdocp = caterwaul.clone(\'std\')(function () {\n\nIntroduction.\nThis driver file governs the process of rendering SDoc source as HTML. The mechanism to achieve this is fairly simple: First, the you download the sdoc.html file from this directory (hosted at\nhttp://spencertipping.com/sdoc/sdoc.html). The HTML file contains instructions for creating a new project; basically, it involves running \'sdoc -p\' and then creating <script> tags to refer to\nthe SDocP files.\n\nThe goal of this project isn\'t to produce immaculate or even particularly well-formatted documentation. Nor is it designed to be flexible. Rather, it\'s (1) to encourage developers (especially\nme) to write well-documented code by making documentation trivial to write, and (2) to provide utility to anyone learning the codebase. SDoc in general is geared towards solving (1), and this\nHTML driver is designed to solve (2). There are several things it does to help:\n\n| 1. Sectioning code. Any sections the developer indicated will be preserved and promoted into proper collapsible sections.\n  2. Indexing. Each word in the code is fully searchable and indexed; it is very easy to find other occurrences of the word and search for particular usage patterns.\n  3. Looking cool. The spiffier the interface looks, the more likely developers are to spend time documenting things :)\n\n  Process.\n  Internally, a couple of things need to happen. First, we need to know when all of the scripts have been loaded. This is easily achieved by comparing the original list of <script>s in the\n  body to the files provided in the sdocp callbacks. Once we have each file we can start parsing, sectioning, and building indexes. (And if after some time we don\'t have all of the files then\n  we can display a warning to the user.)\n\nLogical processing.\nThe first step is to process the SDoc files into a logical structure. This is then used to build the UI and search interface.\n\n  Parsing.\n  This is probably the simplest part of the whole process. Parsing SDoc is as simple as splitting the input into paragraphs and determining the role of each one. Because of some heuristics I\'ll\n  be using later on (see \'Sectioning\'), we need to preserve any whitespace preceding the paragraph\'s first line. The format ends up being an array of snippets, where each snippet is an object of\n  the form {text: \'...\', role: \'comment\' | \'source\'}.\n\n    var parse_sdoc = fn[sdoc][caterwaul.util.map(fn[s][{text: s, role: /^\\s*[A-Z]/.test(s) ? \'comment\' : /^\\s*\\|/.test(s) ? \'pipe\' : \'source\'}], sdoc.split(/\\n\\n+/))],\n\n  Sectioning.\n  Next we need to determine the sections in a document. Informally, sections are \'short lines that start paragraphs and end in a dot\'. For example, the \'Sectioning\' line at the top of this\n  paragraph starts a section. The level of sectioning is determined by indentation -- two spaces per level. Some heuristics are used here. First, the section line must be at least 10\n  characters shorter than the next one; otherwise, it\'s probably just a line-wrap. Second, it must start with a capital letter instead of a pipe symbol. (I\'ll get to what the pipe symbols mean\n  in a bit.)\n\n  A snippet is marked as having sections like this: {text: \'...\', role: \'comment\', section: \'Foo\', level: 2}, where \'level\' starts at 1.\n\n      mark_section = fn[s][let[pieces = /^(\\s*)([A-Z].*)\\.\\n(\\s*)((?:.|\\n)*)$/.exec(s.text)] in (s.role === \'comment\' && pieces && pieces[2].length + 10 < pieces[4].length &&\n                                                                                                 (s.text = pieces[3] + pieces[4], s.section = pieces[2], s.level = 1 + (pieces[1].length >> 1))), s],\n\n  Pipe-paragraph inference.\n  Paragraphs that start with pipes are interesting, because the user had to indicate that they wanted documentation in a situation where the text for some reason looks like code. Most of the\n  time it really is code being presented in example context, but sometimes it\'s something else. It could be:\n\n  | 1. A numbered list (like this one, ha!)\n    2. ASCII art\n    3. A code example\n    4. A continuation of a previous paragraph (but don\'t use | for that!)\n\n  I can\'t think of anything else it\'s likely to be at the moment. So, with these things in mind, here are the heuristics I\'m using for inference:\n\n  | 1. Numbered lists start with /\\d{1,2}\\.\\s{1,2}[A-Za-z]/. That is, one or two digits, a dot, and one or two spaces followed by a letter.\n    2. ASCII art starts with three or more spaces.\n    3. Code snippets start with anything else. (This is why you shouldn\'t use | for normal text.)\n\n  Snippets come out with semantic information: numbered lists have the \'enumerate\' role and an \'items\' array, where the values are the text (newlines and other whitespace intact). Note that\n  the numbering is not smart; it won\'t follow you if you have skips. For example, if you number a list \'2, 3, 5, 7, 11, 13\', it will give you a list numbered as \'2, 3, 4, 5, 6, 7\'. This is\n  partially due to an HTML limitation, and partially due to the fact that I\'m too lazy to accommodate this bizarre case. ASCII art and code snippets are the same; each gets its role changed to\n  \'pre\', and the text loses its pipe symbol.\n\n         mark_pipe = fn[s][s.role === \'pipe\' && (number ? (s.role = \'enumerate\', s.start = Number(number[1]), s.items = [],\n                                                           s.text.replace(/(\\n|^)\\s*(\\d{1,2})\\.\\s{1,2}([A-Za-z](?:.|\\n\\s*[^0-9])*)/g, fn[_, ds, t][s.items.push(t), _])) :\n                                                          (s.role = \'pre\', s.text = s.text.replace(/^(\\s*)\\|/, \'$1 \')),\n                                                 where[number = /^\\s*\\|?\\s*(\\d{1,2})\\.\\s{1,2}[A-Za-z]/.exec(s.text)])],\n\n  Indexing.\n  This is cool. We go through each snippet and construct an index of every word and word pair, optionally separated by up to three words in between. These indexes are mapped into a hashtable\n  from words to the \'relevance\' of that pattern, where relevance is summed over matches, and each match\'s relevance is 1 over the number of words in between plus 1. So, for example, the text\n  \'foo bar bif baz\' matched on the term \'foo bif\' has relevance 0.5.\n\n             index = function (s) {var ws = s.text.split(/[^-\\w]+/), x = s.index = {}; for (var i = 0, l = ws.length; i < l; ++i)\n                                                                                        {x[ws[i]] = (x[ws[i]] || 0) + 1;\n                                                                                         for (var j = i + 1, w = \'#{ws[i]}:#{ws[j]}\'; j < i + 4 && j < l; ++j) x[w] = (x[w] || 0) + 1 / (j - i)}},\n\n  Hierarchical sectioning.\n  Sub-sectioning is done by assigning a \'subsnippets\' array to each snippet with children. The first paragraph of any section is treated kind of specially; it isn\'t moved into a subsnippet\n  like subsequent paragraphs; rather, it\'s stored directly with the section heading.\n\n   fold_subsections = fn[s, ss, i][s && ss[i] ? ss[i].section ? ss[i].level <= s.level ? i : ((s.subsnippets = s.subsnippets || []).push(ss[i]),\n                                                                                              fold_subsections(s, ss, fold_subsections(ss[i], ss, i + 1))) :\n                                                                ((s.subsnippets = s.subsnippets || []).push(ss[i]), fold_subsections(s, ss, i + 1)) : i],\n\n  File sectioning.\n  Each file is broken down into sections. Once we have the contents of each file, we replace its raw text by the hierarchical arrangement of sections. The file\'s section \'level\' is assumed to\n  be zero.\n\n       process_file = fn[filename, text][let[snippets = parse_sdoc(text), map = caterwaul.util.map] in\n                                         (map(mark_section, snippets), map(mark_pipe, snippets), map(index, snippets),\n                                          fold_subsections(result, snippets, 0), result, where[result = {section: filename, level: 0, text: \'\'}])],\n\nRendering.\nOnce files have been broken down into hierarchical sections, we can render them as HTML. This is fairly straightforward; most of the markup is derived directly from the structure of the\nsnippets. Later on I\'ll put in some code to actually process the text and insert semantic markup there.\n\n        html_escape = fn[s][s.replace(/</g, \'&lt;\').replace(/>/g, \'&gt;\').replace(/&/g, \'&amp;\')],\n     render_snippet = fn[s][s.section ? \'<section><h#{s.level + 1}>#{html_escape(s.section)}</h#{s.level + 1}>\' +\n                                                 \'<p>#{html_escape(s.text)}</p>#{map(render_snippet, s.subsnippets || []).join("\\n")}</section>\' :\n                     s.role === \'pre\' ? \'<pre>#{html_escape(s.text)}</pre>\' :\n                  s.role === \'source\' ? \'<pre class="source">#{html_escape(s.text)}</pre>\' :\n               s.role === \'enumerate\' ? \'<ol start="#{s.start}">#{map(fn[s]["<li>" + html_escape(s) + "</li>"], s.items)}</ol>\' :\n                                        \'<p>#{html_escape(s.text)}</p>\', where[map = caterwaul.util.map]],\n           expected = null,\n             loaded = {},\n\n             render = function () {for (var k in loaded) $(\'body\').append(render_snippet(process_file(k, loaded[k])))},\n       check_loaded = function () {for (var i = 0, l = expected.length, all = true; i < l; ++i) all &= ! (expected[i] in loaded); return all};\n\n  return fn[filename, sdoc][expected || (expected = $(\'body > script[src$=".sdocp"]\').map(fn_[$(this).attr(\'src\').replace(/p$/, \'\')])),\n                            loaded[filename] = sdoc, check_loaded() && render()]})();\n');